<!DOCTYPE html>
<html lang="english">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>mathletix | Articles by casey durfee</title>
    <link rel="shortcut icon" type="image/png" href="/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="mathletix Full Atom Feed" />
    <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />
    <link rel="stylesheet" href="/theme/css/pygments.css" type="text/css" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="casey durfee" />
</head>
<body>
    <header>
        <nav style="overflow: hidden;">
            <ul>

                <li class="ephemeral selected"><a href="/author/casey-durfee2.html">casey durfee</a></li>
                <li><a href="/">Home</a></li>
                <li><a href="/pages/about.html">About</a></li>
                <li><a href="/pages/best-of.html">Best Of</a></li>
            </ul>
        </nav>
        <div class="header_box" style="height: 50px">
        <h1><a href="/">
            <image src='' class="avatar" width="50px" /><span class="site_title">mathletix</span>
            </a></h1></div>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Jun 08, 2025</h4>

            <article class="post">
                <h2 class="title">
                    <a href="/approximate-normality-and-continuity-corrections.html" rel="bookmark" title="Permanent Link to &quot;Approximate Normality and Continuity Corrections&quot;">Approximate Normality and Continuity Corrections</a>
                </h2>

                
                

                <p>(Notebooks and other code available at: <a href="https://github.com/csdurfee/hot_hand">https://github.com/csdurfee/hot_hand</a>. As usual, there is stuff in there I'm not covering here.)</p>
<h1>What is "approximately normal"?</h1>
<p>In the last installment, I looked at NBA game-level player data, which involve very small samples.</p>
<p>Like a lot of things in statistics, the Wald Wolfowitz test says that the number of streaks is approximately normal. What does that mean in practical terms? How approximately are we talking?</p>
<p>The number of streaks is a discrete value (0,1,2,3,...). In a small sample like 2 makes and 3 misses, which will be extremely common in player game level shooting data, how could that be <em>approximately normal</em>?</p>
<p>Below is a bar chart of the exact probabilities of each number of streaks, overlaid with the normal approximation in white. Not very normal, is it?</p>
<p><img alt="not very normal" src="/img/not-very-normal.png"></p>
<p>To make things more interesting, let's say the player made 7 shots and missed 4. That's enough for the graph to look more like a proper bell curve.</p>
<p><img alt="exact 7-4 (or 4-7)" src="/img/exact-7-4.png"></p>
<p>The bell curve looks skewed relative to the histogram, right? That's what happens when you model a discrete distribution (the number of streaks) with a continuous one -- the normal distribution.</p>
<p>A continuous distribution has zero probability at any single point, so we calculate the area under the curve between a range of values. The bar for exactly 7 streaks should line up with the probability of between 6.5 and 7.5 streaks in the normal approximation. The curve should be going through the middle of each bar, not the left edge.</p>
<p>We need to shift the curve to the right a half a streak for things to line up. Fixing this is called <a href="https://en.wikipedia.org/wiki/Continuity_correction">continuity correction</a>.</p>
<p>Here's the same graph with the continuity correction applied:</p>
<p><img alt="with cc" src="/img/with-cc.png"></p>
<p>So... better, but there's still a problem. The normal approximation will assign a nonzero probability to impossible things. In this case of 7 makes and 4 misses, the minimum possible number of streaks is 2 and the max is 9 (alternate wins and losses till you run out of losses, then have a string of wins at the end.)</p>
<p>Yet the normal approximation says there's a nonzero chance of -1, 10, or even a million streaks. The odds are tiny, but the normal distribution never ends. These differences go away with big sample sizes, but they may be worth worrying about for small sample sizes.</p>
<p>Is that interfering with my results? It's quite possible. I'm trying to use the mean and the standard deviation to decide how "weird" each player is in the form of a z score. The z score gives the likelihood of the data happening by chance, given certain assumptions. If the assumptions don't hold, the z score, and using it to interpret how <em>weird</em> things are, is suspect.</p>
<h2>Exact-ish odds</h2>
<p>We can easily calculate the exact odds. In the notebook, I showed how to calculate the odds with brute force -- generate all permutations of seven 1's and four 0's, and measure the number of streaks for each one. That's impractical and silly, since the exact counting formula can be worked out using the rules of combinatorics, as this page nicely shows:  <a href="https://online.stat.psu.edu/stat415/lesson/21/21.1">https://online.stat.psu.edu/stat415/lesson/21/21.1</a></p>
<p>In order to compare players with different numbers of makes and misses, we'd want to calculate a percentile value for each one from the exact odds. The percentiles will be based on number of streaks, so 1st percentile would be super streaky, 99th percentile super un-streaky.</p>
<p>Let's say we're looking at the case of 7 makes and 4 misses, and are trying to calculate the percentile value that should go with each number of streaks.  Here are the exact odds of each number of streaks:</p>
<div class="highlight"><pre><span></span><code><span class="mf">2</span><span class="w">    </span><span class="mf">0.006061</span>
<span class="mf">3</span><span class="w">    </span><span class="mf">0.027273</span>
<span class="mf">4</span><span class="w">    </span><span class="mf">0.109091</span>
<span class="mf">5</span><span class="w">    </span><span class="mf">0.190909</span>
<span class="mf">6</span><span class="w">    </span><span class="mf">0.272727</span>
<span class="mf">7</span><span class="w">    </span><span class="mf">0.227273</span>
<span class="mf">8</span><span class="w">    </span><span class="mf">0.121212</span>
<span class="mf">9</span><span class="w">    </span><span class="mf">0.045455</span>
</code></pre></div>

<p>Here are the cumulative odds (the odds of getting that number of streaks or fewer):</p>
<div class="highlight"><pre><span></span><code><span class="mf">2</span><span class="w">    </span><span class="mf">0.006061</span>
<span class="mf">3</span><span class="w">    </span><span class="mf">0.033333</span>
<span class="mf">4</span><span class="w">    </span><span class="mf">0.142424</span>
<span class="mf">5</span><span class="w">    </span><span class="mf">0.333333</span>
<span class="mf">6</span><span class="w">    </span><span class="mf">0.606061</span>
<span class="mf">7</span><span class="w">    </span><span class="mf">0.833333</span>
<span class="mf">8</span><span class="w">    </span><span class="mf">0.954545</span>
<span class="mf">9</span><span class="w">    </span><span class="mf">1.000000</span>
</code></pre></div>

<p>Let's say we get 6 streaks. Exactly 6 streaks happens 27% of the time. 5 or fewer streaks happens 33% of the time. So we could say 6 streaks is equal to the 33rd percentile, the <code>33.3%+27.3% = 61</code>st percentile, or some value in between those two numbers.</p>
<p>The obvious way of deciding the <a href="https://en.wikipedia.org/wiki/Percentile_rank">percentile rank</a> is to take the average of the upper and lower values, in this case <code>mean(.333, .606) = .47</code>. You could also think of it as taking the probability of <code>streaks &lt;=5</code> and adding half the probability of <code>streaks=6</code>.</p>
<p>If we want to compare the percentile ranks from the exact odds to Wald-Wolfowitz, we could convert them to an equivalent z score. Or, we can take the z-scores from the Wald Wolfowitz test and convert them to percentiles.</p>
<p>The two are bound to be a little different because the normal approximation is a bell curve, whereas we're getting the percentile rank from a linear interpolation of two values.</p>
<p>Here's an illustration of what I mean. This is a graph of the percentile ranks vs the CDF of the normal approximation.</p>
<p><img alt="cdf-normal-exact2" src="/img/cdf-normal-exact2.png"></p>
<p>Let's zoom in on the section between 4.5 and 5.5 streaks. Where the white line hits the red line is the percentile estimate we'd get from the z-score (.475).</p>
<p><img alt="cdf-zoom" src="/img/cdf-zoom.png"></p>
<p>The green line is a straight line that represents calculating the percentile rank. It goes from the middle of the top of the <code>runs &lt;= 5</code> bar to the middle of the top of the <code>runs &lt;=6</code> bar. Where it hits the red line is the average of the two, which is percentile rank (.470).</p>
<p>In other situations, the Wald-Wolfowitz estimate will be less than the exact percentile rank. We can see that on the first graph. The green lines and white line are very close to each other, but sometimes the green is higher (like at runs=4), and sometimes the white is higher (like at runs=8).</p>
<h2>Is Wald-Wolfowitz unbiased?</h2>
<p>Yeah. The test provides the exact expected value of the number of streaks. It's not just a pretty good estimate. It is the (weighted) mean of the exact probabilities.</p>
<p>From the exact odds, the mean of all the streak lengths is 6.0909:</p>
<div class="highlight"><pre><span></span><code>count    330.000000
mean       6.090909
std        1.445329
min        2.000000
25%        5.000000
50%        6.000000
75%        7.000000
max        9.000000
</code></pre></div>

<p>The Wald-Wolfowitz test says the expected value is 1 plus the harmonic mean of 7 and 4, which is 6.0909... on the nose.</p>
<h2>Is the normal approximation throwing off my results?</h2>
<p>Quite possibly. So I went back and calculated the percentile ranks for every player-game combo over the course of the season.</p>
<p>Here's a scatter plot of the two ways to calculate the percentile on actual NBA player games. The dots above the x=y line are where the Wald-Wolfowitz percentile is bigger than the percentile rank one.</p>
<p><img alt="percentile-vs-ww" src="/img/percentile-vs-ww.png"></p>
<p>59% of the time, the Wald-Wolfowitz estimate produces a higher percentile value than the percentile rank. The same trend occurs if I restrict the data set to only high volume shooters (more than 10 makes or misses on the game).</p>
<p>Here's a bar chart of the differences between the W-W percentile and the percentile rank:</p>
<p><img alt="ww-minus-pr" src="/img/ww-minus-pr.png"></p>
<p>A percentile over 50, or a positive z score, means more streaks than average, thus less streaky than average. In other words, <em>on this specific data set</em>, the Wald-Wolfowitz z-scores will be more un-streaky compared to the exact probabilities.</p>
<h2>Interlude: our un-streaky king</h2>
<p>For the record, the un-streakiest NBA game of the 2023-24 season was by Dejounte Murray on 4/9/2024. My dude went 12 for 31 and managed 25 streaks, the most possible for that number of makes and misses, by virtue of never making 2 shots in a row.</p>
<p>It was a crazy game all around for Murray. A 29-13-13 triple double with 4 steals, and a Kobe-esque 29 points on 31 shots. He could've gotten more, too. The game went to double overtime, and he missed his last 4 in a row. If he had made the 2nd and the 4th of those, he could've gotten 4 more streaks on the game.</p>
<p>The summary of the game doesn't mention this exceptional achievement. Of course they wouldn't. There's no clue of it in the box score. You couldn't bet on it. Why would anyone notice?</p>
<p><a href="https://www.basketball-reference.com/boxscores/202404090ATL.html">box score on bbref</a>    </p>
<p>Look at that unstreakiness. Isn't it beautiful?</p>
<div class="highlight"><pre><span></span><code><span class="n">makes</span><span class="w">                                                  </span><span class="mi">12</span>
<span class="n">misses</span><span class="w">                                                 </span><span class="mi">19</span>
<span class="n">total_streaks</span><span class="w">                                          </span><span class="mi">25</span>
<span class="n">raw_data</span><span class="w">                  </span><span class="n">LWLWLWLWLWLWLLWLLLWLWLWLWLWLLLL</span>
<span class="n">expected_streaks</span><span class="w">                                </span><span class="mf">15.709677</span>
<span class="n">variance</span><span class="w">                                         </span><span class="mf">6.722164</span>
<span class="n">z_score</span><span class="w">                                          </span><span class="mf">3.583243</span>
<span class="n">exact_percentile_rank</span><span class="w">                           </span><span class="mf">99.993423</span>
<span class="n">z_from_percentile_rank</span><span class="w">                           </span><span class="mf">3.823544</span>
<span class="n">ww_percentile</span><span class="w">                                   </span><span class="mf">99.983032</span>
</code></pre></div>

<p>On the other end, the streakiest performance of the year belonged to Jabari Walker of the Portland Trail Blazers. Made his first 6 shots in a row, then missed his last 8 in a row.</p>
<div class="highlight"><pre><span></span><code><span class="n">makes</span><span class="w">                                  </span><span class="mi">6</span>
<span class="n">misses</span><span class="w">                                 </span><span class="mi">8</span>
<span class="n">total_streaks</span><span class="w">                          </span><span class="mi">2</span>
<span class="n">raw_data</span><span class="w">                  </span><span class="n">WWWWWWLLLLLLLL</span>
<span class="n">expected_streaks</span><span class="w">                </span><span class="mf">7.857143</span>
<span class="n">variance</span><span class="w">                        </span><span class="mf">3.089482</span>
<span class="n">z_score</span><span class="w">                        </span><span class="o">-</span><span class="mf">3.332292</span>
<span class="n">exact_percentile_rank</span><span class="w">             </span><span class="mf">0.0333</span>
<span class="n">z_from_percentile_rank</span><span class="w">         </span><span class="o">-</span><span class="mf">3.403206</span>
<span class="n">ww_percentile</span><span class="w">                   </span><span class="mf">0.043067</span>
</code></pre></div>

<h2>Actual player performances</h2>
<p>Let's look at actual NBA games where a player had exactly 7 makes and 4 misses. (We can also include the flip side, 4 makes and 7 misses, because it will be the same distribution of streak lengths)</p>
<p>The green areas are where the players had more streaks than the exact probabilities; the red areas are where players had fewer streaks. The two are very close, except for a lot more games with 9 streaks in the player data, and fewer 6 streak games.</p>
<p>The exact mean is 6.09 streaks. The mean for player performances is 6.20 streaks. Even in this little slice of data, there's a slight tendency towards unstreakiness.</p>
<p><img alt="streaks-vs-probs" src="/img/streaks-vs-probs.png"></p>
<h2>Percentile ranks are still unstreaky, though</h2>
<p>Well, for all that windup, the game-level percentile ranks didn't turn out all that different when I calcualted them for all 18,000+ player-game combos. The mean and median are still shifted to the un-streaky side, to a significant degree.</p>
<p><img alt="z-from-percentile" src="/img/z-from-percentile.png"></p>
<p>Plotting the deciles shows an interesting tendency: a lot more values in the 60-70th percentile range than expected. the shift to the un-streaky side comes pretty much from these values.</p>
<p><img alt="perc-rank-deciles" src="/img/perc-rank-deciles.png"></p>
<p>The bias towards the unstreaky side is still there, and still significant:</p>
<div class="highlight"><pre><span></span><code>count    18982.000000
mean         0.039683
std          0.893720
min         -3.403206
25%         -0.643522
50%          0.059717
75%          0.674490
max          3.823544
</code></pre></div>

<h2>A weird continuity correction that seems obviously bad</h2>
<p>SAS, the granddaddy of statistics software, applies a continuity correction to the runs test whenever the count is less than 50.</p>
<p>While it's true that we should be careful with normal approximations and small sample size, this ain't the way.</p>
<p>The exact code used is here: <a href="https://support.sas.com/kb/33/092.html">https://support.sas.com/kb/33/092.html</a></p>
<div class="highlight"><pre><span></span><code><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nv">N</span><span class="w"> </span><span class="nv">GE</span><span class="w"> </span><span class="mi">50</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nv">Z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">(</span><span class="nv">Runs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">mu</span><span class="ss">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">sigma</span><span class="c1">;</span>
<span class="w">        </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">Runs</span><span class="o">-</span><span class="nv">mu</span><span class="w"> </span><span class="nv">LT</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nv">Z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">(</span><span class="nv">Runs</span><span class="o">-</span><span class="nv">mu</span><span class="o">+</span><span class="mi">0</span>.<span class="mi">5</span><span class="ss">)</span><span class="o">/</span><span class="nv">sigma</span><span class="c1">;</span>
<span class="w">          </span><span class="k">else</span><span class="w"> </span><span class="nv">Z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">(</span><span class="nv">Runs</span><span class="o">-</span><span class="nv">mu</span><span class="o">-</span><span class="mi">0</span>.<span class="mi">5</span><span class="ss">)</span><span class="o">/</span><span class="nv">sigma</span><span class="c1">;</span>
</code></pre></div>

<p>Other implementations I looked at, like the one in R's <a href="https://www.rdocumentation.org/packages/randtests/versions/1.0.1/topics/runs.test"><code>randtests</code> package</a>, don't do the correction.</p>
<p>What does this sort of correction look like?</p>
<p>For starters, it gives us something that doesn't look like a z score. The std is way too small.</p>
<div class="highlight"><pre><span></span><code>count    18982.000000
mean        -0.031954
std          0.687916
min         -3.047828
25%         -0.401101
50%          0.000000
75%          0.302765
max          3.390395
</code></pre></div>

<p><img alt="sas-cc" src="/img/sas-cc.png"></p>
<h3>What does this look like on random data?</h3>
<p>It could just be this dataset, though. I will generate a fake season of data like in the last installment, but the players will have no unstreaky/streaky tendencies. They will behave like a coin flip, weighted to their season FG%. So the results should be distributed like we expect z scores to be (mean=0, std=1)</p>
<p>Here are the z-scores. They're not obviously bad, but the center is a bit higher than it should be.</p>
<p><img alt="sas-sim" src="/img/sas-sim.png"></p>
<p>However, the continuity correction especially stands out when looking at small sample sizes (in this case, simulated players with fewer than 30 shooting streaks over the course of the season).</p>
<p>In the below graph, red are the SAS corrected z-scores, green are the wald-wolfowitz z scores, brown are the overlap.</p>
<p><img alt="sas-low-vol" src="/img/sas-low-vol.png"></p>
<p>Continuity corrections are at best an imperfect substitute for calculating the exact odds. These days, there's no reason not to use exact odds for smaller sample sizes. Even though it ended up not mattering much, I should've started with the percentile rank for individual games. However, I don't think that the game level results are as important to the case I'm making as the career-long shooting results.</p>
<p>Next time, I will look at the past 20 years of NBA data. Who is the un-streakiest player of all time?</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="/approximate-normality-and-continuity-corrections.html">posted at 10:20</a>
                    &nbsp;&middot;&nbsp;<a href="/category/statistics.html" rel="tag">statistics</a>
                    &nbsp;&middot;
                    &nbsp;<a href="/tag/basketball.html" class="tags">basketball</a>
                    &nbsp;<a href="/tag/the-hot-hand.html" class="tags">the hot hand</a>
                </div>
            </article>            <h4 class="date">May 28, 2025</h4>

            <article class="post">
                <h2 class="title">
                    <a href="/simulating-hot-and-lukewarm-hands.html" rel="bookmark" title="Permanent Link to &quot;Simulating hot and lukewarm hands&quot;">Simulating hot and lukewarm hands</a>
                </h2>

                
                

                <p>(Notebooks and other code available at: <a href="https://github.com/csdurfee/hot_hand">https://github.com/csdurfee/hot_hand</a>. There's a bunch of stuff in the notebook about the Wald-Wolfowitz test that I will save for another week.)</p>
<p>In my last installment, I was looking at season long shooting records from the NBA, and I concluded that NBA players were less streaky than expected. They have fewer long strings of makes and misses than a series of coin flips would.</p>
<p>I've been thinking this could be due to "heat check" shots -- a player has made a bunch of shots in a row, or are having a good shooting game in general, so they take harder shots than they normally take. 
It would explain some players that fans consider streaky or "heat check" players who are actually super un-streaky. Jordan Poole was the least streaky player over the last 4 seasons, which defies my expectations. Say he believes he is streaky, so tends to take bad shots when</p>
<p>Or it could be due to "get a bucket" shots -- a player is having a bad shooting game, so they force higher percentage shots and potentially free throws. </p>
<p>There's a quirk of NBA stats to remember: if a player is fouled while shooting, it only counts as a field goal attempt if they make the shot. So driving to the hoop is guaranteed to not decrease a player's field goal percentage if they successfully draw a foul, or get called for an offensive foul.</p>
<p>I'm not sure I've made an airtight case for the <em>lukewarm hand</em>. Combining every game in a season could hide the hot hand effect. What about individual games?</p>
<h2>Game-level shooting statistics show a lukewarm tendency</h2>
<p>I am using the complete shooting statistics available from this kaggle project: <a href="https://www.kaggle.com/datasets/mexwell/nba-shots">https://www.kaggle.com/datasets/mexwell/nba-shots</a></p>
<p>I'm looking at the 2023-2024 season, since the current season isn't included yet.</p>
<p>I went through every game that every player played in the NBA season and calculated the expected vs. actual number of streaks.  </p>
<p>There are 24,895 player+game combos. 10,285 of them had more streaks than expected against 8,977 who had fewer streaks than expected (and around 5,000 that are exactly as expected). This is a significant imbalance towards the "lukewarm hand" side.</p>
<p>Here's the histogram of individual game z-scores:</p>
<p><img alt="individual game z-scores, 2023" src="/img/game-zscores-2023.png"></p>
<p>And the breakdown:</p>
<div class="highlight"><pre><span></span><code><span class="nx">count</span><span class="w">    </span><span class="m m-Double">18982.000000</span>
<span class="nx">mean</span><span class="w">         </span><span class="m m-Double">0.051765</span>
<span class="nx">std</span><span class="w">          </span><span class="m m-Double">0.988789</span>
<span class="nx">min</span><span class="w">         </span><span class="o">-</span><span class="m m-Double">3.332292</span>
<span class="mi">25</span><span class="o">%</span><span class="w">         </span><span class="o">-</span><span class="m m-Double">0.707107</span>
<span class="mi">50</span><span class="o">%</span><span class="w">          </span><span class="m m-Double">0.104103</span>
<span class="mi">75</span><span class="o">%</span><span class="w">          </span><span class="m m-Double">0.816497</span>
<span class="nx">max</span><span class="w">          </span><span class="m m-Double">3.583243</span>
<span class="nx">Name</span><span class="p">:</span><span class="w"> </span><span class="nx">z_score</span><span class="p">,</span><span class="w"> </span><span class="nx">dtype</span><span class="p">:</span><span class="w"> </span><span class="nx">float64</span>
</code></pre></div>

<p>Limiting to higher volume games (at least 10 makes or 10 misses) shows the same tendency.</p>
<p><img alt="high attempt games, 2023-24" src="/img/high-attempt-games.png"></p>
<div class="highlight"><pre><span></span><code><span class="nx">count</span><span class="w">    </span><span class="m m-Double">2536.000000</span>
<span class="nx">mean</span><span class="w">        </span><span class="m m-Double">0.055925</span>
<span class="nx">std</span><span class="w">         </span><span class="m m-Double">1.010195</span>
<span class="nx">min</span><span class="w">        </span><span class="o">-</span><span class="m m-Double">3.079575</span>
<span class="mi">25</span><span class="o">%</span><span class="w">        </span><span class="o">-</span><span class="m m-Double">0.616678</span>
<span class="mi">50</span><span class="o">%</span><span class="w">         </span><span class="m m-Double">0.072404</span>
<span class="mi">75</span><span class="o">%</span><span class="w">         </span><span class="m m-Double">0.750366</span>
<span class="nx">max</span><span class="w">         </span><span class="m m-Double">3.583243</span>
<span class="nx">Name</span><span class="p">:</span><span class="w"> </span><span class="nx">z_score</span><span class="p">,</span><span class="w"> </span><span class="nx">dtype</span><span class="p">:</span><span class="w"> </span><span class="nx">float64</span>
</code></pre></div>

<p>There definitely appears to be a bias towards the lukewarm hand in individual game data. The mean z scores aren't that much bigger than zero, but it's a huge sample size.</p>
<h2>Simulating streaky and non-streaky players</h2>
<p>I coded up a simulation of a non-streaky player. When they have hit a minimum number of attempts in the game, if their shooting percentage goes above a certain level, they get a penalty. If it goes below a certain level, they get a boost.</p>
<p>I was able to create results that look like NBA players in aggregate with an extremely simplified model. The parameters were arbitrarily chosen</p>
<p>By default, the thresholds are 20% and 80%, and the boost/penalty is 20%. So a 50% shooter who has taken at least 4 shots and is shooting 80% or better for the game will get their FG% knocked down to 30% till their game percentage drops below the threshold. Likewise if they hit 20% or less, they get a boost until they're over the threshold.</p>
<p>I used the game level shooting statistics I got for the individual game-by-game analysis. I then replayed every shot in the NBA in the 2023-24 season using the simulated lukewarm player (and the actual fg% and number of shots attempted in each game). This is what I got:</p>
<p><img alt="sim-z-scores" src="/img/sim-z-scores.png"></p>
<div class="highlight"><pre><span></span><code><span class="nx">count</span><span class="w">    </span><span class="m m-Double">526.000000</span>
<span class="nx">mean</span><span class="w">       </span><span class="m m-Double">0.218032</span>
<span class="nx">std</span><span class="w">        </span><span class="m m-Double">0.965737</span>
<span class="nx">min</span><span class="w">       </span><span class="o">-</span><span class="m m-Double">2.397958</span>
<span class="mi">25</span><span class="o">%</span><span class="w">       </span><span class="o">-</span><span class="m m-Double">0.491051</span>
<span class="mi">50</span><span class="o">%</span><span class="w">        </span><span class="m m-Double">0.241554</span>
<span class="mi">75</span><span class="o">%</span><span class="w">        </span><span class="m m-Double">0.836839</span>
<span class="nx">max</span><span class="w">        </span><span class="m m-Double">3.787951</span>
<span class="nx">Name</span><span class="p">:</span><span class="w"> </span><span class="nx">z_score</span><span class="p">,</span><span class="w"> </span><span class="nx">dtype</span><span class="p">:</span><span class="w"> </span><span class="nx">float64</span>
</code></pre></div>

<p>My simulation was actually less biased to the right than the actual results:</p>
<p><img alt="actual-2023-24" src="/img/actual-2023-24.png"></p>
<p>Several big things to note:</p>
<ol>
<li>I simulated every player in the league as being a little un-streaky.</li>
<li>I simulated them being un-streaky in both directions</li>
<li>The boost/penalty are pretty big -- going from 50% FG percentage to 30% is going from a good NBA player to a bad college player level, and the boost to 70% FG percentage has no precedent. The most accurate shooters in the NBA are usually big men who only shoot dunks and layups, and they still usually end up in the 60-65% range.</li>
</ol>
<p>Which is to say, my simulation is kind of silly and seemingly over-exaggerated. And it's still not as lukewarm as real NBA players are.  Wild, isn't it?</p>
<h2>Streakiness in only one direction</h2>
<p>I also simulated players who were only streaky in one direction: "get a bucket" players who get a boost to shooting percentage when they are shooting poorly, but no penalty when they are doing well, and "heat check" players who only get the penalty.</p>
<p>The results were biased to the unstreaky side, but about half as much as the ones that are streaky in both directions. I had to crank the penalties/boosts up to unrealistic levels to get the bias of the z-scores up to the .2-.3 range I'm seeing with real season-level data.</p>
<h2>The truly streaky player</h2>
<p>Of course, I had to simulate the hot hand. The <code>TrulyStreakyPlayer</code> is the exact opposite of the <code>LukewarmPlayer</code>. They get a 20% boost when they're shooting well on the game, and a 20% penalty when they're shooting poorly.</p>
<p>What stands out to me here is how much it affects the z-score. I was expecting the z-scores to be biased to the negative side by about as much as the unstreaky player was to the positive side. But the effect was a lot more dramatic:</p>
<div class="highlight"><pre><span></span><code>count    524.000000
mean      -0.455522
std        1.144570
min       -4.413268
25%       -1.225128
50%       -0.458503
75%        0.404549
max        2.486584
</code></pre></div>

<p><img alt="truly-streaky-player" src="/img/truly-streaky-player.png"></p>
<p>Unlike the un-streaky simulations, the streaky behavior increased the dispersion (<code>std</code>), like we saw with the real shot data. There are many more outliers to the negative side than we'd expect.</p>
<h2>What next?</h2>
<p>I could certainly sim a mixture of streaky and unstreaky players, and eventually maybe get something that matches the real numbers pretty closely. But there are so many parameters to fit that it would be pretty arbitrary. Someone else could produce a different model that works just as well. </p>
<p>Most importantly, it couldn't tell us which players might be streaky due to chance versus streaky due to behavior/shot selection. So I think the next step is looking at the shot selection in the "hot hand" vs. "get a bucket" situations -- do players switch to higher percentage shots when they're having a bad game, and worse shots when they're shooting better than usual?</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="/simulating-hot-and-lukewarm-hands.html">posted at 10:20</a>
                    &nbsp;&middot;&nbsp;<a href="/category/sports-analytics.html" rel="tag">sports analytics</a>
                    &nbsp;&middot;
                    &nbsp;<a href="/tag/basketball.html" class="tags">basketball</a>
                    &nbsp;<a href="/tag/the-hot-hand.html" class="tags">the hot hand</a>
                </div>
            </article>            <h4 class="date">May 23, 2025</h4>

            <article class="post">
                <h2 class="title">
                    <a href="/what-are-the-most-important-events-at-the-nfl-combine.html" rel="bookmark" title="Permanent Link to &quot;What are the most important events at the NFL Combine?&quot;">What are the most important events at the NFL Combine?</a>
                </h2>

                
                

                <p>(the code used is available at <a href="https://github.com/csdurfee/nfl_combine_data/">https://github.com/csdurfee/nfl_combine_data/</a>).</p>
<h2>Intro</h2>
<p>Every year, the National Football League hosts an event called the Combine, where teams can evaluate the top prospects before the upcoming draft.</p>
<p>Athletes are put through a series of physical and mental tests over the course of four days. There is a lot of talk of hand size, arm length, and whether a guy looks athletic enough when he's running with his shirt off. It's basically the world's most invasive job interview. </p>
<p>NFL teams have historically put a lot of stock in the results of the combine. A good showing at the combine can improve a player's career prospects, and a bad showing can significantly hurt them. For that reason, some players will opt out of attending the combine, but that can backfire as well.</p>
<p>I was curious about which events in the combine correlate most strongly with draft position. There are millions of dollars at stake. The first pick in the NFL draft gets a $43 Million dollar contract, the 33rd pick gets $9.6 Million, and the 97th pick gets $4.6 Million.</p>
<p>The main events of the combine are the 40 yard dash, vertical leap, bench press, broad jump, 3 cone drill and shuttle drill. The shuttle drill and the 3 cone drill are pretty similar -- a guy running between some cones as fast as possible. The other drills are what they sound like.</p>
<p>I'm taking the data from Pro Football Reference. Example page: <a href="https://www.pro-football-reference.com/draft/2010-combine.htm">https://www.pro-football-reference.com/draft/2010-combine.htm</a>. I'm only looking at players who got drafted.</p>
<h2>Position Profiles</h2>
<p>It makes no sense to compare a cornerback's bench press numbers to a defensive lineman's. There are vast differences in the job requirements. A player in the combine is competing against other players at the same position. </p>
<p>The graph shows a position's performance on each exercise relative to all players. The color indicates how the position as a whole compares to the league as a whole. You can change the selected position with the dropdown.</p>
<iframe src="/img/position_view_2025.html" width="550" height="450" frameborder="0"></iframe>

<p>Cornerbacks are exceptional on the 40 yard dash and shuttle drills compared to NFL athletes as a whole, whereas defensive linemen are outliers when it comes high bench press numbers, and below average at every other event. Tight Ends and Linebackers are near the middle in every single event, which makes sense because both positions need to be strong enough to deal with the strong guys, and fast enough to deal with the fast guys.</p>
<h2>Importance of Events by Position</h2>
<p>I analyzed how a player's performance relative to others at their position correlates with draft rank. Pro-Football-Reference has combine data going back to 2000.  I have split the data up into 2000-2014 and 2015-2025 to look at how things have changed. </p>
<p>For each position, the exercises are ranked from most to least important. The tooltip gives the exact r^2 value.</p>
<p>Here are the results up to 2014:</p>
<iframe src="/img/before_2015.html" width="800" height="700" frameborder="0"></iframe>

<p>Here are the last 10 years:</p>
<iframe src="/img/after_2015.html" width="800" height="700" frameborder="0"></iframe>

<p>Some things I notice:</p>
<p>The main combine events matter that much either way for offensive and defensive linemen. That's held true for 25 years.</p>
<p>The shuttle and 3 cone drill have gone up significantly in importance for tight ends.</p>
<p>Broad jump and 40 yard dash are important for just about every position. However, the importance of the 40 yard dash time has gone down quite a bit for running backs. </p>
<p>As a fan, it used to be a huge deal when a running back posted an exceptional 40 yard time. It seemed Chris Johnson's legendary 4.24 40 yard time was referenced every year. But I remember there being lot of guys who got drafted in the 2000's primarily based on speed who turned out to not be very good. </p>
<p>The bench press is probably the least important exercise across the board. There's almost no correlation between performance and draft order, for every position. Offensive and defensive linemen basically bench press each other for 60 minutes straight; for everybody else, that sort of strength is less relevant.  Here's one of the greatest guys at throwing the football in human history, Tom Brady:</p>
<p><img alt="behold" src="/img/Tom-Brady-Combine.png"></p>
<p>Compared to all quarterbacks who have been drafted since 2000, Brady's shuttle time was in the top 25%, his 3 cone time was in the top 50%, and his broad jump, vertical leap and 40 yard dash were all in the bottom 25%.</p>
<h2>Changes in combine performance over time</h2>
<p>Athlete performance has changed over time. </p>
<p>I've plotted average performance by year for each of the events. For the 40 yard dash, shuttle, and 3 cone drills, lower is better, and for the other events, higher is better.</p>
<p><img alt="change over time" src="/img/combine-trends.png"></p>
<p>40 yard dash times and broad jump distances have clearly improved, whereas shuttle times and bench press reps have gotten slightly worse.</p>
<p>There's a cliche in sports that "you can't coach speed".  While some people are innately faster than others, the 40 yard dash is partly a skill exercise -- learning to get off the block as quickly as possible without faulting, for starters. The high priority given to the 40 yard dash should lead to prospects practicing it more, and thus getting better numbers.</p>
<p>The bench press should be going down or staying level, since it's not very important to draft position.</p>
<p>There's been a significant improvement in the broad jump - about 7.5% over 25 years. As with the 40 yard dash, I'd guess it's better coaching and preparation. Perhaps it's easier to improve than some of the other events. I don't think there's more broad jumping in an NFL game than there was 25 years ago.</p>
<p>Shuttle times getting slightly worse is a little surprising. It's very similar to the 3 Cone drill, which has slightly improved. But as we saw, neither one is particularly important as far as draft position, and it's not a strong trend.</p>
<h2>Caveats</h2>
<p>Some of the best athletes skip the combine entirely, because their draft position is already secure. And some athletes will only choose to do the exercises they think they will do well at, and skip their weak events. This is known as <a href="https://www.ncbi.nlm.nih.gov/books/NBK493614/">MNAR data</a> (missing, not at random). All analysis of MNAR data is potentially biased.</p>
<p>I'm assuming a linear relationship between draft position and performance. It's possible that a good performance helps more than a bad performance hurts, or vice versa.</p>
<p>I didn't calculate statistical significance for anything. Some correlations will occur even in random data. This isn't meant to be rigorous.</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="/what-are-the-most-important-events-at-the-nfl-combine.html">posted at 10:20</a>
                    &nbsp;&middot;&nbsp;<a href="/category/sports-analytics.html" rel="tag">sports analytics</a>
                    &nbsp;&middot;
                    &nbsp;<a href="/tag/football.html" class="tags">football</a>
                </div>
            </article>            <h4 class="date">May 22, 2025</h4>

            <article class="post">
                <h2 class="title">
                    <a href="/majority-voting-in-ensemble-learning.html" rel="bookmark" title="Permanent Link to &quot;Majority Voting in Ensemble Learning&quot;">Majority Voting in Ensemble Learning</a>
                </h2>

                
                

                <p>(notebook is available at <a href="https://github.com/csdurfee/ensemble_learning/blob/main/ensemble_voting.ipynb">github.com/csdurfee/ensemble_learning</a>.)</p>
<h2>Ensemble Learning</h2>
<p>AI and machine learning systems are often used for classification. Is this email spam or not? Is this person a good credit risk or not? Is this a photo of a cat or not?</p>
<p>There are a lot of ways to build classifiers, and they all potentially have different strengths and weaknesses. It's natural to try combining multiple models together to produce better results than the individual models would. </p>
<p>Model A might be bad at classifying black cats but good at orange ones, model B might be bad at classifying orange cats but good at black ones, model C is OK at both. So if we average together the results of the three classifiers, or go with the majority opinion between them, the results might be better than the individual classifiers.</p>
<p>This is called an ensemble. Random forests and gradient boosting are two popular machine learning techniques that use ensembles of <em>weak learners</em> -- a large number of deliberately simple models that are all trained on different subsets of the data. This strategy can lead to systems that are more powerful than their individual components. While each little tree in a random forest is weak and prone to overfitting, the forest as a whole can be robust and give high quality predictions.</p>
<h2>Majority Voting</h2>
<p>We can also create ensembles of <em>strong learners</em> -- combining multiple powerful models together. Each individual model is powerful enough to do the entire classification on its own, but we hope to achieve higher accuracy by combining their results.  The most common way to do that is with voting. Query several classifiers, and have the ensemble return the majority pick, or otherwise combine the results.</p>
<p>There are some characteristics of ensembles that seem pretty common sense [1]. The classifiers in the ensemble need to be <em>diverse</em>: as different as possible in the mistakes they make. If they all make the same mistakes, then there's no way for the ensemble to correct for that. </p>
<p>The more classification categories, the more classifiers are needed in the ensemble. However, in real world settings, there's usually a point where adding more classifiers doesn't improve the ensemble. </p>
<h2><a name="model"></a>The Model</h2>
<p>I like building really simple models. They can illustrate fundamental characteristics, and show what happens at the extremes. </p>
<p>So I created an extremely simple model of majority voting (see <a href="https://github.com/csdurfee/ensemble_learning/blob/main/ensemble_voting.ipynb">notebook</a>). I'm generating a random list of 0's and 1's, indicating the ground truth of some binary classification problem. Then I make several copies of the ground truth and randomly flip <code>x%</code> of the bits. Each of those copies represent the responses from an individual classifier within the ensemble. Each fake classifier will have <code>100-x%</code> accuracy. There's no correlation between the wrong answers that each classifier gives, because the changes were totally random. </p>
<p>For every pair of fake classifiers with 60% accuracy, they will both be right <code>60% * 60% = 36%</code> of the time, and both wrong <code>40% * 40% = 16%</code>. So they will agree <code>36% + 16% = 52%</code> of the time at minimum.</p>
<p>That's different from the real world. Machine learning algorithms trained on the same data will make a lot of the same mistakes and get a lot of the same questions right. If there are outliers in the data, any classifier can overfit on them. And they're all going to find the same basic trends in the data. If there aren't a lot of good walrus pictures in the training data, every model is probably going to be bad at recognizing walruses. There's no way to make up for what isn't there.</p>
<h2>Theory vs Reality</h2>
<p>In the real world, there seem to be limits on how much an ensemble can improve classification. On paper, there are none, as the simulation shows.</p>
<p>What is the probability of the ensemble being wrong about a particular classification?</p>
<p>That's the probability that the majority of the classifiers predict 0, given that the true value is 1 (and vice versa). If each classifier is more likely to be right than wrong, as the number of classifiers goes to infinity, the probability of the majority of predictions being wrong goes to 0.</p>
<p>If each binary classifier has a probability &gt; .5 of being right, we can make the ensemble arbitrarily precise if we add enough classifiers to the ensemble (assuming their errors are independent). We could grind the math using the normal approximation to get the exact number if need be.</p>
<p>Let's say each classifier is only right 50.5% of the time. We might have to add 100,000 of them to the ensemble, but we can make the error rate arbitrarily small.</p>
<h2>Correlated errors ruin ensembles</h2>
<p>The big difference between my experiment and reality is that the errors the fake classifiers make are totally uncorrelated with each other. I don't think that would ever happen in the real world. </p>
<p>The more the classifiers' wrong answers are correlated with each other, the less useful the ensemble becomes. If they are 100% correlated with each other, the ensemble will give the exact same results as the individual classifiers, right? An ensemble doesn't <em>have to</em> improve results.</p>
<p>To put it in human terms, the "wisdom of the crowd" comes from people in the crowd having wrong beliefs about uncorrelated things (and being right more often than not overall). If most people are wrong in the same way, there's no way to overcome that with volume. </p>
<p>My experience has been that different models tend to make the same mistakes, even if they're using very different AI/machine learning algorithms, and a lot of that is driven by weaknesses in the training data used.</p>
<p>For a more realistic scenario, I created fake classifiers with correlated answers, so that they agree with the ground truth 60% of the time, and with each other 82% of the time, instead of the minimum 52% of the time. 
The Cohen kappa score is .64, on a scale from -1 to 1, so they aren't as correlated as they could be.</p>
<p>The simulation shows that if the responses are fairly strongly correlated with each other, there's a hard limit to how much the ensemble can improve things. </p>
<p>Even with 99 classifiers in the ensemble, the simulation only achieves an f1 score of .62. That's just a slight bump from the .60 achieved individually. There is no marginal value to adding more than 5 classifiers to the ensemble at this level of correlation.</p>
<h2>Ensembles: The Rich Get Richer</h2>
<p>I've seen voting ensembles suggested for especially tricky classification problems, where the accuracy of even the  best models is pretty low. I haven't found that to be true, though, and the simulation backs that up. Ensembles are only going to give a significant boost for binary classification if the individual classifiers are significantly better than 50% accuracy.</p>
<p>The more accurate the individual classifiers, the bigger the boost from the ensemble. These numbers are for an ensemble of 3 classifiers (in the ideal case of no correlation between their responses):</p>
<table>
<thead>
<tr>
<th>Classifier Accuracy</th>
<th>Ensemble Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>55%</td>
<td>57%</td>
</tr>
<tr>
<td>60%</td>
<td>65%</td>
</tr>
<tr>
<td>70%</td>
<td>78%</td>
</tr>
<tr>
<td>80%</td>
<td>90%</td>
</tr>
</tbody>
</table>
<h2>Hard vs. soft voting</h2>
<p>There are two different ways of doing majority voting, hard and soft. This choice can have an impact on how well an ensemble works, but I haven't seen a lot of guidance on when to use each. </p>
<p><em>Hard voting</em> is where we convert the outputs of each binary classifier into a boolean yes/no, and go with the majority opinion. If there are an odd number of components and it's a binary classification, there's always going to be a clear winner. That's what I've been simulating so far.</p>
<p><em>Soft voting</em> is where we combine the raw outputs of all the components, and then round the combined result to make the prediction. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier">sklearn's documentation</a> advises to use soft voting "for an ensemble of well-calibrated classifiers".</p>
<p>In the real world, binary classifiers don't return a nice, neat 0 or 1 value. They return some value between 0 or 1 indicating a relative level of confidence in the prediction, and we round that value to 0 or 1. A lot of models will never return a 0 or 1 -- for them, nothing is impossible, just extremely unlikely.</p>
<p>If a classifier returns <code>.2</code>, we can think of it as the model giving a <code>20%</code> chance that the answer is <code>1</code> and an <code>80%</code> chance it's <code>0</code>. That's not really true, but the big idea is that there's potentially additional context that we're throwing away by rounding the individual results.</p>
<p>For instance, say the raw results are <code>[.3,.4,.9]</code>. With hard voting, these would get rounded to <code>[0,0,1]</code>, so it would return <code>0</code>. With soft voting, it would take the average of <code>[.3,.4,.9]</code>, which is <code>.53</code>, which rounds to <code>1</code>. So the two methods can return different answers.</p>
<p>To emulate the soft voting case, I flipped a percentage of the bits, as before. Then I replaced every 0 with a number chosen randomly from the uniform distribution from <code>[0,.5]</code> and every <code>1</code> with a sample from <code>[.5,1]</code>. The values will still round to what they did before, but there's additional noise on top. </p>
<p>In this simulation (3 classifiers), the soft voting ensemble gives less of a boost than the hard voting ensemble -- about half the benefits. As with hard voting, the more accurate the individual classifiers are, the bigger the boost the ensemble gives. </p>
<table>
<thead>
<tr>
<th>Classifier Accuracy</th>
<th>Ensemble Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>55%</td>
<td>56%</td>
</tr>
<tr>
<td>60%</td>
<td>62%</td>
</tr>
<tr>
<td>70%</td>
<td>74%</td>
</tr>
<tr>
<td>80%</td>
<td>85%</td>
</tr>
</tbody>
</table>
<h2>Discussion</h2>
<p>Let's say a classifier returns <code>.21573</code> and I round that down to 0. How much of the <code>.21573</code> that got lost was noise, and how much was signal? If a classification task is truly binary, it could be all noise. Let's say we're classifying numbers as odd or even. Those are unambiguous categories, so a perfect classifier should always return exactly 0 or 1. It shouldn't say that three is odd, with 90% confidence. In that case, it clearly means the classifier is 10% wrong. There's no good reason for uncertainty.</p>
<p>On the other hand, say we're classifying whether photos contain a cat or not. What if a cat is wearing a walrus costume in one of the photos? Shouldn't the classifier return a value greater than 0 for the possibility of it not being a cat, even if there really is a cat in the photo? Isn't it somehow less cat-like than another photo where it's not wearing a walrus costume? In this case, the <code>.21573</code> at least partially represents signal, doesn't it? It's saying "this is pretty cat-like, but not as cat-like as another photo that scored <code>.0001</code>".</p>
<p>When I'm adding noise to emulate the soft voting case, is that <em>fair</em>? A different way of fuzzing the numbers (selecting the noise from a non-uniform distribution, for instance) might reduce the gap in performance between hard and soft voting ensembles, and it would probably be more realistic. But the point of a model like this is to show the extremes -- it's possible that hard voting will give better results than soft voting, so it's worth testing.</p>
<h2>Big Takeaways</h2>
<ol>
<li>Ensembles aren't magic; they can only improve things significantly if the underlying classifiers are diverse and fairly accurate.</li>
<li>Hard and soft voting aren't interchangeable. If there's a lot of random noise in the responses, hard voting is probably a better option, otherwise soft voting is probably better. It's definitely worth testing both options when building an ensemble.</li>
<li>Anyone thinking of using an ensemble should look at the amount of correlation between the responses from different classifiers. If the classifiers are all making basically the same mistakes, an ensemble won't help regardless of hard vs. soft voting. If models with very different architectures are failing in the same ways, that could be a weakness in the training data that can't be fixed by an ensemble.</li>
</ol>
<h3>References</h3>
<p>[1] Bonab, Hamed; Can, Fazli (2017). "Less is More: A Comprehensive Framework for the Number of Components of Ensemble Classifiers". arXiv:1709.02925</p>
<p>Tsymbal, A., Pechenizkiy, M., &amp; Cunningham, P. (2005). Diversity in search strategies for ensemble feature selection. Information Fusion, 6(1), 83–98. doi:10.1016/j.inffus.2004.04.003 </p>
                <div class="clear"></div>

                <div class="info">
                    <a href="/majority-voting-in-ensemble-learning.html">posted at 10:20</a>
                    &nbsp;&middot;&nbsp;<a href="/category/machine-learning.html" rel="tag">machine learning</a>
                </div>
            </article>            <h4 class="date">May 16, 2025</h4>

            <article class="post">
                <h2 class="title">
                    <a href="/the-hot-hand-doesnt-exist-in-the-nba-but-its-opposite-does.html" rel="bookmark" title="Permanent Link to &quot;The hot hand doesn't exist in the NBA, but its opposite does&quot;">The hot hand doesn't exist in the NBA, but its opposite does</a>
                </h2>

                
                

                <p>(The code used, and ipython notebooks with a fuller investigation of the data is available at <a href="https://github.com/csdurfee/hot_hand">https://github.com/csdurfee/hot_hand</a>.)</p>
<h1>Streaks</h1>
<p>When I'm watching a basketball game, sometimes it seems like a certain player just can't miss. Every shot looks like it's going to go in. Other times, it seems like they've gone cold. They can't get a shot to go in no matter what they do. </p>
<p>This phenomenon is known as the "hot hand" and whether it exists or not has been debated for decades, even as it's taken for granted in the common language around sports. We're used to commentators saying that a player is  "heating up", or, "that was a heat check".</p>
<p>As a fan of the game, it certainly seems like the hot hand exists. If you follow basketball, some names probably come to mind. JR Smith, Danny Green, Dion Waiters, Jamal Crawford. When they're on, they just can't miss. It doesn't matter how crazy the shot is, it's going in. And when they're cold, they're <em>cold</em>.</p>
<p>It's a thing we collectively believe in, but it turns out that there isn't clear statistical evidence to support it.</p>
<p>We have to be careful with our feelings about the hot hand. It certainly feels real, but that doesn't mean that it is. Within the drama of a basketball game, we're inclined to notice and assign stories to runs of makes or misses. Just because we notice them, that doesn't mean they're significant. This is sometimes called "the law of small numbers" -- our brains have a tendency to reach spurious conclusions from a very small amount of data.</p>
<p><a href="https://en.wikipedia.org/wiki/Pareidolia">Pareidolia</a> is the human tendency to see human faces in inanimate objects -- clouds, the bark of a tree, a tortilla. While the faces might seem real, they are just a product of our brain's natural inclination to identify patterns. It's possible the "hot hand" is a similar phenomenon -- a product of the way human brains are wired to see patterns, rather than an objective truth.</p>
<h2>Defining Streakiness</h2>
<p>Streaks of 1's and 0's in randomly generated binary data follow regular mathematical laws, ones our brains can't realy replicate. Writer <a href="https://www.football-data.co.uk/blog/Wald_Wolfowitz.php">Joseph Buchdal</a> found that he couldn't create a random-looking sequence by hand that would fool a statistical test called the Wald-Wolfowitz test, even though he knew exactly how the statistical test worked.</p>
<p>I think at some level, we're physically incapable of generating truly random data, so it makes sense to me that our intuitions about randomness are a little off. Our brains are wired to notice the streaks, but we seem to have no such circuitry for noticing when something is a little bit too un-streaky. Our brains are too quick to see meaningless patterns in small amounts of data, and not clever enough to see subtle, meaningful patterns in large amounts of data. Good thing we have statistics to help us escape those biases!</p>
<p>For the sake of this discussion, a streak starts whenever a sequence of outcomes changes from wins (W) to losses (L), or vice-versa. (I'm talking about makes and misses, but those start with the same letter, so I'll use "W" and "L".)</p>
<p>The sequence <code>WLWLWL</code> has 6 streaks: <code>W, L, W, L, W, L</code>      <br>
The sequence <code>WWLLLW</code> has 3 streaks: <code>WW, LLL, W</code></p>
<p>Imagine I asked someone to produce a random-looking string of 3 W's and 3 L's. If they were making the results up, I think the average person would be more likely to write the first string. It just looks "more random", right? </p>
<p>If they flipped a coin, it would be more likely to produce something with longer streaks, like the second example. With a fair coin, both of those <em>exact</em> sequences are equally likely to occur. But the second sequence has a more probable number of streaks, according to the <a href="https://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test">Wald-Wolfowitz Runs Test</a>. The expected number in 3 wins and 3 losses is (2 * (3 * 3) / (3+3)) + 1 = 4.</p>
<p>The expected number of streaks is the <a href="https://en.wikipedia.org/wiki/Harmonic_mean">harmonic mean</a> of the number of wins and the number of losses, plus one. Neat, right?</p>
<p>Around 500 players attempted a shot in the NBA this season. Let's say we create a custom coin for each player. It comes up heads with the same percentage as the player's shooting percentage on the season. If we took those coins and simulated every shot in the NBA this season, some of the coins would inevitably appear to be "streakier" than others.</p>
<p>Players never intend to miss shots, yet most players shoot around 50%, so there has to be some element of chance as far as which shots go in or not. Otherwise, why wouldn't players just choose to make all of them?</p>
<p>So makes versus misses are at least somewhat random, which means if we look at the shooting records of 500 players in an NBA season, some will seem more or less consistent due to the laws of probability. That means a player with longer or shorter streaks than expected could just be due to chance, not due to the player actively doing something that makes them more streaky.</p>
<h2>The Lukewarm Hand</h2>
<p>We might call players who have fewer streaks than expected by chance <em>consistent</em>. Maybe they go exactly 5 for 10 every single game, never being especially good or especially bad. Or maybe they go 1 for 3 every game, always being pretty bad.</p>
<p>But that feels like the wrong word, and I don't think our brains aren't really wired to notice a player that has fewer streaks than average. As we already saw, the "right" number of streaks is counterintuitive. </p>
<p>I might notice a player is unusually consistent after the fact when looking at their basketball-reference page, but the feeling of a player having the <em>hot hand</em> is visceral, experienced in the moment. Even without consulting the box score, sometimes players look like they just can't miss, or can't make, a shot. They seem more confident, or their shot seems more natural, than usual. Both the shooter and the spectator seem to have a higher expectation that the shot will go in than usual. The hot hand is a social phenomenon.</p>
<p><img alt="there's always an xkcd" src="/img/xkcd-sports.png">  <br>
(from <a href="https://xkcd.com/904/">https://xkcd.com/904/</a>)</p>
<p>If we look at the makes and misses of every player in the league, do they look like the results of flipping a coin (weighted to match their shooting percentage), or is there a tendency for players to be more or less streaky than expected by chance? </p>
<p>We don't really have a formal word for players who are less streaky than they should be, so I'm going to call the opposite of the <em>hot hand</em> the <em>lukewarm hand</em>. While the <em>lukewarm hand</em> isn't a thing we would viscerally notice the way we do the <em>hot hand</em>, it's certainly possible to exist. And it's just as surprising, from the perspective of treating basketball players like weighted coins. </p>
<p>Some people I've seen analyze the hot hand treat the question as <em>streaky</em> versus <em>non-streaky</em>. But it's not a binary thing. There are two possible extremes, and a region in between. It's <em>unusually streaky</em> versus <em>normal amount of streaky</em> versus <em>unusually non-streaky</em>.</p>
<p>The Wald-Wolfowitz test says that the number of streaks in randomly-generated data will be normally distributed, and gives a formula for the variance of the number of streaks. The normal distribution is symmetrical, so there should be as many <em>hot hand</em> players as <em>lukewarm hand</em> ones. Players have varying numbers of shots taken over the course of the season so we can't compare them directly, but we can calculate the z score for each player's expected vs. actual number of streaks. The z score represents how "weird" the player is. If we look at all the z-scores together, we can see whether NBA players as a whole are streakier or less streaky than chance alone would predict. We can also see if the outliers correspond to the popular notions of who the streaky shooters in the NBA are.</p>
<h2>Simplifying Assumptions</h2>
<p>We should start with the assumption that athletes really are weighted random number generators. A coin might have "good days" and "bad days" based on the results, but it's not because the coin is "in the zone" one day, or a little injured the next day. At least some of the variance in a player's streakiness is due to randomness, so we have to be looking for effects that can't be explained by randomness alone.</p>
<p>So I am analyzing all shots a player took, across all games. This could cause problems, which I will discuss later on, but splitting the results up game-by-game or week-by-week leads to other problems. Looking at shooting percentages by game or by week means smaller sample sizes, and thus more sampling error. It also means that comparisions between high volume shooters and low volume shooters can be misinterpreted. The high volume shooters may appear more "consistent" simply because it's a larger sample size.</p>
<p>I think I need to prove that <em>streakiness</em> exists before making assumptions about how it works. Let's say the "hot hand" does exist. If a player makes a bunch of shots in a row, how long might they stay hot? Does it last through halftime? Does it carry over to the next game? How many makes in a row before they "heat up"? How much does a player's field goal percentage go up? Does a player have cold streaks and hot streaks, or are they only streaky in one direction?</p>
<p>There are an infinite number of ways to model how it could work, which means it's ripe for overfitting. So I wanted to start with the simplest, most easily justifiable model. The <a href="https://www.sciencedirect.com/science/article/abs/pii/0010028585900106?via%3Dihub">original paper about the hot hand</a> was co-written by Amos Tversky, who went on to win a Nobel Prize for helping to invent behavioral economics. I figure any time you can crib off of a Nobel Prize winner's homework, you probably should!</p>
<h2>Results</h2>
<p>I started off by getting data on every shot taken in the 2024-25 NBA regular season. I calculated the expected number of streaks and actual number, then a z-score for every player.</p>
<p>Players with a z-score of 0 are just like what we'd expect from flipping a coin. A positive z-score indicates there were more streaks than expected. More streaks than expected means the streaks were shorter than expected, which means less streaky than expected. </p>
<p>A negative z-score indicates the opposite. Those players had fewer streaks than expected, which means the streaks were longer.  When people talk about the "hot hand" or "streaky shooters", they are talking about players who should have a negative z-score by this test.</p>
<p><img alt="all players, 2024-5" src="/img/all-players-2024.png"></p>
<p>The curve over the top is the distribution of z-scores we'd expect if the players worked like weighted coin flips. </p>
<p>Just eyeballing it, it's pretty close. It's definitely a bell curve, centered pretty close to zero. If there is a skew, it's actually to the positive, un-streaky side, though.  The mean z-score is .21, when we'd expect it to be zero.</p>
<div class="highlight"><pre><span></span><code>count    554.000000
mean       0.212491
std        1.075563
min       -3.081194
25%       -0.546340
50%        0.236554
75%        0.951653
max        3.054836
</code></pre></div>

<p>The Wilk-Shapiro test is way to decide whether a set of data plausibly came from a normal distribution. It passed. There is no conclusive evidence that players in general are streakier or less streaky than predicted by chance. This data very well could've come from flipping a bunch of coins.</p>
<p>But it's still sorta skewed. There were 320 players with a positive z-score (un-streaky) versus 232 with a negative z-score (streaky). That's suspicious.</p>
<h2>Outliers</h2>
<p>A whole lot of those 554 players didn't make very many shots.  </p>
<p><img alt="numer of makes, 2024-5" src="/img/makes-2024.png"></p>
<p>I decided to split up players with over 100 makes versus under 100 makes. Unlike high volume shooters, the low volume shooters had no bias towards unstreakiness. They look like totally random data.</p>
<p>Here are just the high volume shooters (323 players in total). Notice how none of them have a z-score less than about -2. It should be symmetrical.</p>
<p><img alt="over 100 makes, 2024-5" src="/img/over-100-makes.png"></p>
<div class="highlight"><pre><span></span><code>count    323.000000
mean       0.347452
std        1.068341
min       -2.082528
25%       -0.454794
50%        0.363949
75%        1.091244
max        3.054836
</code></pre></div>

<p>There were 20 players with a z-score less than -2 versus only 2 players with a score greater than 2.</p>
<h2>The Eye Test</h2>
<p>I looked at which players had exceptionally high or low z scores. The names don't really make sense to me as an NBA fan. There were players like Jordan Poole and Jalen Green, who I think fans would consider streaky, but they had exceptionally un-streaky z-scores. I don't think the average NBA fan would say Jalen Green is less streaky than 97.5% of the players in the league, but he is (by this test).</p>
<p>On the other hand, two streakiest players in the NBA this year were Goga Bitadze and Thomas Bryant, two players who don't fit the profile of the stereotypical streaky shooter by any means.</p>
<h2>Makes vs. Streakiness</h2>
<p>The more shots a player made this season, the less streaky they tended to be. Here's a plot of makes on the 2024-25 season versus the z-score.</p>
<p><img alt="makes vs z-score" src="/img/makes-vs-zscore.png"></p>
<p>That's pretty odd, isn't it?</p>
<h2>Getting more data: 2021-present</h2>
<p>I figured a bigger sample size would be better. Maybe this season was just weird. So got the last 4 seasons of data (2021-22,2022-2023, 2023-2024, 2024-2025) for players who made a shot in the NBA this season and combined them.</p>
<p>The four year data is even more skewed towards the <em>lukewarm hand</em>, or un-streaky side, than the single year data.</p>
<p><img alt="all players, 2021-2025" src="/img/all-players-four-year.png"></p>
<div class="highlight"><pre><span></span><code>count    562.000000
mean       0.443496
std        1.157664
min       -4.031970
25%       -0.312044
50%        0.449647
75%        1.184918
max        4.184025
</code></pre></div>

<p>The correlation between number of makes and z-score is quite strong in the 4 year data:</p>
<p><img alt="2021-2025 z score vs makes" src="/img/four-year-makes-vs-zscore.png"></p>
<p>There were 48 players with a z-score &gt; 2, versus only 9 with a score &lt;-2. That's like flipping a coin and getting 48 heads and 9 tails. There's around a 2 in 10 million chance of that happening with a fair coin.</p>
<h2>High Volume Shooters, Redux</h2>
<p>The bias towards the lukewarm hand is even stronger among high volume shooters. Here are players with more than 500 makes over the past 4 years.</p>
<p><img alt="over 500 makes" src="/img/over-500-makes.png"></p>
<p>The z-scores are normally distributed according to the Wilk-Shapiro test, but they're no longer even close to being centered at zero. They're also overdispersed (the std is bigger than the expected 1.) It's not plausible that the true mean is 0, given the sample mean is .680.</p>
<div class="highlight"><pre><span></span><code>count    265.000000
mean       0.680097
std        1.217946
min       -2.392061
25%       -0.149211
50%        0.776917
75%        1.485595
max        4.184025
</code></pre></div>

<p><img alt="high volume hist" src="/img/high-volume-hist.png"></p>
<h2>Streak Lengths</h2>
<p>I looked at the length of make/miss streaks for the actual NBA players versus simulating the results. The results were simulated by taking the exact number of makes and misses for each NBA player, and then shuffling those results randomly. What I found confirmed the "lukewarm hand" -- overall, NBA players have slightly more 1 and 2 shot streaks than expected, and fewer long streaks than expected.</p>
<p><img alt="streaks" src="/img/streaks.png"></p>
<h2>Obvious objections, and what about free throws?</h2>
<p>I'm treating every field goal attempt like it has the same chance of going in. Clearly that's not the case. Players, especially high volume scorers, can choose which shots they take. It's easy to imagine a player that has missed several shots in a row and is feeling "cold" would concentrate on only taking higher percentage shots. There's also the fact that I'm combining games together. That could potentially lead to players looking less streaky than they are within the course of a single game. But it should also make truly unstreaky players look less <em>unstreaky</em>. Streaks getting "reset" by the end of the game should make players act more like a purely random process -- not too streaky or unstreaky. It shouldn't increase the standard deviation of the z-scores like we're seeing, or cause a shift towards unstreakiness. </p>
<p>I may do a simulation to illustrate that, but in the meantime, the most controlled shot data we have is free throw data. Every free throw should have exactly the same level of difficulty for the player. </p>
<p>I got the data for the 200,000+ free throws in the NBA regular season over the past four years (October 2021 through April 2025).</p>
<p>Here are the z-scores for all players. There's a big chunk taken out of the middle of the bell curve, but it's normal-ish other than that.</p>
<p><img alt="free throws" src="/img/free-throws.png"></p>
<p>240 players have made over 200 free throws in the past 4 years. When I restrict to just those players, there's a slight skew towards the "hot hand", or being more streaky than expected. There are no exceptionally <em>lukewarm hands</em> when it comes to free throws. It's sort of the mirror image of what we saw with high volume field goal shooters.</p>
<p><img alt="free throws, over 200 makes" src="/img/over-200-ft.png"></p>
<div class="highlight"><pre><span></span><code>count    240.000000
mean      -0.144277
std        1.021330
min       -2.686543
25%       -0.854723
50%       -0.174146
75%        0.660302
max        1.845302
</code></pre></div>

<h2>Conclusions, for now</h2>
<p>I feel comfortable concluding that the hot hand doesn't exist when it comes to field goals. I can't say why there's a tendency towards <em>unstreakiness</em> yet, but I suspect it is due to shot selection. Players who have made a bunch of shots may take more difficult shots than average, and players who have missed a bunch of shots will go for an easier shot than average. While players can't choose when to "heat up" or "go cold", they can certainly change shot selection based on their emotions or the momentum of the game.</p>
<p>There may be a slight tendency towards the hot hand when it comes to free throws. It's worth investigating further, I think. But the effect there doesn't appear to be nearly as strong as the <em>lukewarm hand</em> tendency for field goals.</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="/the-hot-hand-doesnt-exist-in-the-nba-but-its-opposite-does.html">posted at 10:20</a>
                    &nbsp;&middot;&nbsp;<a href="/category/sports-analytics.html" rel="tag">sports analytics</a>
                    &nbsp;&middot;
                    &nbsp;<a href="/tag/basketball.html" class="tags">basketball</a>
                    &nbsp;<a href="/tag/the-hot-hand.html" class="tags">the hot hand</a>
                </div>
            </article>            <h4 class="date">May 01, 2025</h4>

            <article class="post">
                <h2 class="title">
                    <a href="/the-hardest-road.html" rel="bookmark" title="Permanent Link to &quot;The Hardest Road&quot;">The Hardest Road</a>
                </h2>

                
                

                <h2>What geology can tell us about Kevin Durant's next team</h2>
<p>When NBA superstar Kevin Durant left the Oklahoma City Thunder to join the Golden State Warriors, he said that doing so was taking "the hardest road". This was met with a lot of mockery, because the Golden State Warriors had just won 73 games, the most in NBA history, the previous year.</p>
<p>It was widely regarded as an uncool move, ring chasing, the ultimate bandwagon riding. It was clearly an absurd thing to say about the level of challenge he chose. It also made the NBA less interesting for several years, so he deserved some hate for it.</p>
<p>What people missed was that according to geology, he wasn't totally off-base. Streets are paved with asphalt, which is a combination of local rocks (aggregate) and tar. That means that some regions of America have harder roads than others, based on the local geology:</p>
<p><img alt="taken from https://www.forconstructionpros.com/equipment/worksite/article/10745911/aggregate-hardness-map-of-the-united-states" src="/img/aggregate-map.jpg">
(<a href="https://www.forconstructionpros.com/equipment/worksite/article/10745911/aggregate-hardness-map-of-the-united-states">source: https://www.forconstructionpros.com/equipment/worksite/article/10745911/aggregate-hardness-map-of-the-united-states</a>)</p>
<p>Oklahoma City is located right in the center of Oklahoma, with some of the softest aggregate in the United States. It's reasonable for someone who cares about road hardness to want to leave. Just about anywhere (except for Florida) would have been an improvement.</p>
<p>The "hardest road" out of OKC at that time would've been the one to New Orleans. It's about a 700 mile drive, and it looks like it's a nice gradient from some of the softest roads in the United States to the very hardest ones. </p>
<p>The New Orleans Pelicans at the time were <a href="https://www.basketball-reference.com/teams/NOP/2016.html">pretty bad</a>, basically just Anthony Davis, a couple good role players (Ryan Anderson, Jrue Holiday), and a rich collection of "Let's Remember Some Guys" Guys (Jimmer Fredette, Nate Robinson, Luke Babbitt, Ish Smith, Alonzo Gee, Norris Cole). KD and AD on the same team would have been cool, but even with Kevin Durant, the Pelicans would likely have been pretty bad. Certainly worse than the OKC team that Durant wanted to leave.</p>
<p>Although technically the "hardest road" out of Oklahoma City, going to New Orleans would have been a poor career choice for KD. The Pelicans have always been a cheap, poorly run team. I can't imagine it being a destination for any free agent of Kevin Durant's caliber. </p>
<p>He really should have said "I'm taking the hardest road that doesn't lead to a mismanaged tire fire of a team. Also by "hardest" I mean on the <a href="https://en.wikipedia.org/wiki/Mohs_scale">Mohs scale</a>, not the challenge" and everybody would have understood.</p>
<p>Northern California has a medium-hard substrate, so his choice to go to the Warriors was definitely a harder road than a lot of other places he could have gone. Since leaving the Warriors, he's played for two other teams with medium-hard roads: the Brooklyn Nets and the Phoenix Suns. He's never chosen to take a softer road. Give him credit for that. </p>
<p>Now that there are rumors about Kevin Durant being traded from the Suns, what can geology tell us about Durant's next destination?</p>
<p>The other NBA cities with medium-hard to hard roads are New Orleans, Boston, Charlotte, Houston, New York, Sacramento, Utah, and Washington DC. He's from DC so that might be nice. But Durant always says he wants to compete for a championship. So we can rule them out, as well as New Orleans, Charlotte and Utah. </p>
<p>I can't really see Boston or New York wanting to tweak their rosters too much, because they're both already good enough to win a championship and don't have a lot of tradeable assets. Sacramento's not a great fit. The Kings would be dreadful on defense, and have too many players who need the ball at once.</p>
<p>That leaves Houston. Durant would fix the Rockets' biggest weakness -- not having a go-to scorer -- and Houston could surround him with a bunch of guys who can play defense. Most importantly, he'd get to continue to drive on medium-hard roads. </p>
<p>Kevin Durant to the Houston Rockets. The geology doesn't lie.</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="/the-hardest-road.html">posted at 10:20</a>
                    &nbsp;&middot;&nbsp;<a href="/category/sports-analytics.html" rel="tag">sports analytics</a>
                    &nbsp;&middot;
                    &nbsp;<a href="/tag/basketball.html" class="tags">basketball</a>
                </div>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="/author/casey-durfee.html" class="prev_page">&larr;&nbsp;Previous</a>


                    <span>Page 2 of 2</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/tcarwash/blue-penguin-dark">Blue Penguin Dark</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>