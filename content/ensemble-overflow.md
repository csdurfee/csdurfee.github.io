
## Hunting Mushrooms In Random Forests
Bootstrapping (or bagging) is an ensemble learning technique where each classifier gets trained on a different subset of the training data, and then the results of each classifier are combined together to make predictions.

The most clever, or most pathological, version of bootstrapping ensembles are random forests, which consist of a large number of simple decision trees. Decision trees are one of the oldest AI algorithms, although these days we'd call it machine learning. I think they're appealing because they match a naive model of how expertise works. Say you want to know if a mushroom is safe to eat. Using a guidebook, you could do a bunch of measurements of the mushroom and follow a set of rules. If it's smaller than 20 centimeters, and the gills are this shape, and it's growing on this type of log, and the spore print is this color, it's safe to eat, or it isn't. The guidebook allows you to do follow the same process that a mushroom expert would follow. The temptation to use decision trees as an AI technique is very understandable. It's kind of AI at its most primal: condense the knowledge of humans down to a purely mathematical set of rules that requires no sentience or true understanding, like the instructions for a perfect [Chinese Room](https://en.wikipedia.org/wiki/Chinese_room).

Unfortunately, decision trees don't work very well. They have a pathological tendency to overfit. They can't help but be painfully literal. They learn the price of everything, and the value of nothing.

Let's say there are two mushrooms that are identical except for size, the big ones are poisonous, the smaller ones aren't.  A decision tree would classify a mushroom as *small* if the radius is less than 12.34298 centimeters, and *big* if it's even a smidge bigger than that. It will have an extremely precise decision boundary between the two categories. 

"Big" and "small" are higher level concepts for a human expert, not exact definitions down to the nanometer. They might not use a ruler to decide whether a mushroom is small or big, but instead some combination of guesstimation, rules of thumb, past experience, higher theoretical knowledge, etc. They kinda *know it when they see it*. It's not one hard and fast rule.

12.34298 might be the statistically optimal place to split the two categories based on the training data, but it's probably not some immutable law of the Universe. Mushrooms probably don't become poisonous once they get bigger than that size, right? So when confronted with a mushroom it has never seen before, the decision tree will follow extremely precise rules, but be much less accurate at predictions than a human that's just eyeballing it. Its *understanding* of mushrooms is too literal. It is built on the assumption that everything that needs to be known about mushrooms is contained in the training data, which must be memorized as perfectly as possible.

Random forests are a way to combat the inherent tendency of decision trees to overfit. Instead of having one big decision tree trained on all the data, it uses a bunch of small decision trees trained on subsets of the data, then averages together the opinions of each tree in the forest. Although the individual decision trees are overfitted, they're all overfitted on different sets of data, and when combined together in an ensemble, their weaknesses cancel each other out. Each of the individual trees in the forest is a *weak learner*. Each individual tree might be only a little better than flipping a coin. But when combined with a bunch of other *weak learners*, they can produce accurate results.

Boosting algorithms are another ensemble approach. Rather than having a bunch of small models and averaging their answers together like random forests, boosting algorithms use a sequence of *weak learners*. Each learner in the chain is trained on the mistakes made in the previous step. It might overfit on one small part of the data, but the next one in the chain can correct for it. Data points that the previous step got wrong are weighted as more important; ones that the previous step got right are weighted as less important. Each step in the chain makes the answers more accurate and robust. This technique works remarkably well. 30 years after the development of the AdaBoost algorithm, it is still quite useful.
